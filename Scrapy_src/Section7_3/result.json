{"contents": "In the fifth and final post of this , we will share with you how we architect a web scraping solution, all the core components of a well-optimized solution, and the resources required to execute it.To give you an inside look at this process in action, we will give you a behind the scenes look at examples of projects we\u2019ve scoped for our clients.But first, let\u2019s take a look at the main components you need for every web scraping project\u2026There are a few core components to every web scraping project that you need to have in place if you want to reliably extract high-quality data from the web at scale:However, depending on your project requirements you might also need to make use of other technologies to extract the data you need:The amount of resources required to develop and maintain the project will be determined by the type and frequency of data needed and the complexity of the project.Talking about the building blocks of web scraping projects is all well and good, however, the best way to see how to scope a solution is to look at real examples.In the first example, we\u2019ll look at one of the most common web scraping use cases - Product Monitoring. Every day Scrapinghub receives numerous requests from companies looking to develop internal product intelligence capabilities through the use of web scraped data. Here we will look at a typical example:The customer wanted to extract product from specific product pages from Amazon.com. They would provide a batch of search terms and the crawlers would search for those keywords and extract all products associated with them (~500 keywords per day).The extracted data will be used in a customer facing product intelligence tool for consumer brands looking to monitor their own products along with the products of their competitors. Typically, extracting product data poses very few legal issues provided that the crawler (1) doesn\u2019t have to scrape behind a login (which often isn\u2019t the case), (2) is only scraping factual or non-copyrightable information, and (3) the client doesn\u2019t want to recreate the target website\u2019s whole store, which may bring database rights into question. In this case, the project had little to no legal challenges.Although this project required a large scale crawling of a complex and ever-changing website, projects like this are Scrapinghub\u2019s bread and butter. We have considerable experience delivering similar (and more complex) projects for clients so this was a very manageable project. We would be able to reuse a considerable amount of code used elsewhere to enable us to get the project up and running very quickly for the client. After assessing the project the solution architect then developed a custom solution to meet the client's requirements. The solution consisted of three main parts:Scrapinghub successfully implemented this project for the client. The crawlers developed now extract ~500,000 products per day from the site, which the client inputs directly into their customer-facing product monitoring application.In the next example, we\u2019re going to take a look at a more complex web scraping project that required us to use artificial intelligence to extract the article data from over 300+ news sources.The customer wanted to develop a news aggregator app that will curate news content for their specific industries and interests. They provided an initial list of 300 news sites they wanted to crawl, however, they indicated that this number was likely to rise as their company grew. The client required every article in specific categories to be extracted from all the target sites, crawling the site every 15 minutes to every hour depending on the time of day. The client needed to extract the following data from every article:Once extracted this data would be fed directly into their customer-facing app so ensuring high quality and reliable data was a critical requirement. With article extraction, you always want to be cognizant of the fact that the articles are copyrighted material of the target website. You must ensure that you are not simply copying an entire article and republishing it. In this case, since the customer was aggregating the content internally and only republishing headlines and short snippets of the content, it was deemed that this project could fall under the fair use doctrine under copyright law. There are various copyright considerations and use cases to take into account when dealing with article extraction, so it is always best to consult with your legal counsel first.Although the project was technically feasible, due to the scale of the project (developing high-frequency crawlers for 300+ websites) the natural concern was that it would be financially unviable to pursue such a project.As a rule of thumb, it takes an experienced crawl engineer 1-2 days to develop a robust and scalable crawler for one website. Doing a rough calculation will quickly show that to manually develop 300+ crawlers would be a very costly project if it required 1 work day per crawler.With this in mind, our solution architecture team explored the use of AI enabled intelligent crawlers that would remove the need to code custom crawlers for every website. After conducting the technical feasibility assessment the solution architect then developed a custom solution to meet the client's requirements. The solution consisted of three main parts:Scrapinghub successfully implemented this project for the client, who now is able to extract 100,000-200,000 articles per day from the target websites for the news aggregation app.So there you have it, this is the four-step process Scrapinghub uses to architect solutions for our client's web scraping projects. At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.If you have a need to start or scale your web scraping project then our  are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."}
{"contents": "In this the third post in our solution architecture series, we will share with you our step-by-step process for conducting a legal review of every web scraping project we work on.At Scrapinghub, it\u2019s absolutely critical that our services respect the rights of the websites and companies whose data we scrape. Scraping, as a process, is not illegal - however, the data you extract, the manner in which you extract the data, and what exactly you\u2019re scraping all need to be held to rigorous legal standards to ensure legal compliance.In ensuring that your solution architecture follows both legal guidelines as well as industry best practices, we\u2019ve established a checklist for your ease and to protect the reputation and integrity of web scraping as a practice. Personal and commercial data regulations are in flux across the world, and given the inherently international nature of the internet, establishing clearly legal practices within your solutions should be considered an executive priority.In this article we will discuss the three critical legal checks you need to make when reviewing the legal feasibility of any web scraping project and the exact questions you should be asking yourself when planning your data extraction needs.Data comes in all shapes and sizes. However, before we start extracting this data, we need to determine the exact status and legality of extracting this data for each project.There are three forms of data that can be present a legal risk if extracted:1. 2. 3. However, the first step of the legal review process is to identify the use case for the data - i.e. what will the data be used for and do you have the data owners explicit consent to extract, store and use their data.The ultimate use case of the data can have a large bearing on the legal status of scraping the data from a website, particularly in the case of personal data which we will discuss later.So the first step of any legal review process is to define:Once this has been defined, you will be in a position to carry out your legal checks.Personal data, or personally identifiable information (PII) as it is technically known, is any data that could be used to directly or indirectly identify a specific individual. With the increased awareness and regulation governing how personal data is used, extracting personal data has resulted in increasingly stringent data protection regulations coming into force - the General Data Protection Regulation, or GDPR, is a prime example.First, you need to check whether you plan to extract any form of personal data. Common examples include:If you\u2019re not extracting any personal data, then you can move onto the next step of the legal review. However, if you are extracting any of the personal data types listed above then you need to investigate the data protection regulations associated with this data.Every legal jurisdiction (US, EU, etc.) has different regulations governing personal data. So the next step is to identify which jurisdiction do the owners of this personal data reside: the EU, US, Canada, etc.For a detailed step-by-step process for evaluating the legal regulations of the personal data you want to extract then be sure to check out our .Copyrighted data generally describes content owned by businesses and individuals with explicit control over its reproduction and capture. Just because web data is publicly available on the internet doesn\u2019t mean that anyone can extract and store the data.In some cases the data itself might be copyrighted, and depending on how/what data you extract you could be found to have infringed the owner\u2019s copyright, creating additional risks for the users of this data.First, you need to check whether you plan to extract any form of data that are at risk of being subject to copyright. Common examples include:If you are extracting any of these forms of web data, then you need to determine if you will violate copyright by extracting and using the data in your projects.Cases like these need to be evaluated on a case-by-case basis as copyright issues often aren\u2019t black and white like personal data issues, they are sometimes surmountable if there is a valid exception to copyright within your use case. Some methods to achieve this are:Database rights is a subset of copyright, that needs further explanation on it\u2019s own. A database is any organized collection of materials that permits a user to search for and access individual pieces of information contained within the materials.Database rights can create additional risks for the use of web data in your projects, if the data hasn\u2019t been extracted in a compliant manner.In the US, a database is protected by copyright when the selection or arrangement is original and creative. Copyright only protects the selection and organization of the data, not the data itself.In the EU, databases are protected under the Database Directive which offers much broader protection for EU databases. The Directive has two purposes: (1) protect IP, like in the US, and (2) protect the work and risk in creating the database.If you believe a data source might fall under database rights then decision makers should always consult with their legal team before scraping the data and ensure they either:Copyright can be a tricky topic, so it is always best to talk to a qualified legal professional prior to scraping potentially copyrightable data for your projects. At Scrapinghub, every web scraping project request we receive is reviewed for copyright issues by our legal team prior to commencing the project. Ensuring our clients know they are extracting data in a legally compliant manner.Extracting data from a website that first requires you to login to access the data can raise potential legal issues. In most situations, logging it requires you to accept the terms and conditions of the website which might explicitly state that automatic data extraction is prohibited.If this is the case, you should review the terms and conditions to determine whether you would be in breach of the T&C\u2019s by extracting data from the website. As the terms and conditions of some of these websites can sometimes be quite intricate, it is advisable that you have them reviewed by an experienced legal professional prior to scraping data from behind the login.In order to maintain compliance with today\u2019s data regulations, it\u2019s incredibly important to keep your legal team up-to-date and to ensure data protection specialists regularly monitor your scraping operation. Legal checks are integral to keeping compliant and demonstrate goodwill - furthermore, by performing consistent legal assessments of your projects, you can streamline the scraping process and make absolutely certain that your scraping remains respectful and productive.As we have seen, there is more to web scraping than just the technical implementation of a project. There are numerous legal compliance requirements that need to be taken into account when deciding if a web scraping project is viable.If the guidelines outlined in this article are followed, then there is no reason why you can\u2019t extract data from the web without exposing yourself to undue compliance and regulatory risks.At Scrapinghub we have extensive experience developing data extraction solutions that overcome these challenges and mitigate the compliance risks associated with using web scraped data in your business.If you have a need to start or scale your web scraping projects then our  are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."}
{"contents": "If you\u2019ve been using Scrapy for any period of time, you know the capabilities a well-designed Scrapy spider can give you.With a couple lines of code you can design a scalable web crawler and extractor that will automatically navigate to your target website and extract the data you need. Be it e-commerce, article or sentiment data.The one issue that traditional Scrapy spiders poses however, is the fact that in a lot of cases spiders can take a long time to finish their crawls and deliver their data if it is a large job. Making Scrapy spiders unsuitable for near real-time web scraping. A growing and very useful application for many data aggregation and analysis efforts.With the growth of data based services and data-driven decision making, end users are increasingly looking for ways to extract data on demand from web pages instead of having to wait for data from large periodic crawls.And that\u2019s where ScrapyRT comes in\u2026Originally evolving out of a  project in 2014,  (Scrapy Realtime) is an open source Scrapy extension that enables you to control Scrapy spiders with HTTP requests.Simply send your Scrapy HTTP API a request containing the  (with URL and callback as parameters) and the API will return the extracted data by the spider in real-time. No need to wait for the entire crawl to complete.Usually, spiders are run for long periods of time, and proceed step by step, traversing the web from a starting point and extracting any data that matches their extraction criteria.This mode of operation is great if you don\u2019t know the location of your desired data. However, if you know the location of the data then there is a huge amount of redundancy if the spider has to complete all the intermediary steps.ScrapyRT allows you to schedule just one single request with spider, parse it in callback, and get response returned immediately as JSON instead of having the data saved in a database.By default spider\u2019s start_requests spider method is not executed and the only request that is scheduled with spider is Request generated from API params.ScrapyRT\u2019s architecture is very simple. It is a web server written in Python Twisted tied with custom Crawler object from Scrapy.Twisted is one of the most powerful Python asynchronous frameworks, so was a natural choice for ScrapyRT as Twisted works great for asynchronous crawling and Scrapy uses Twisted for all HTTP traffic. Ensuring easy integration with Scrapy.Once added to your project, ScrapyRT runs as a web service, retrieving data when you make a request containing the URL you want to extract data from and the name of the spider you would like to use.ScrapyRT will then schedule a request in Scrapy for the URL specified and use the \u2018foo\u2019 spider\u2019s parse method as a callback. The data extracted from the page will be serialized into JSON and returned in the response body. If the spider specified doesn\u2019t exist, a 404 will be returned.The ScrapyRT web server is customizable and modular. You can easily override GET and POST handlers. This means that you can add your own functionality to the project - you can write your own  inheriting from main ScrapyRT handlers, for example you can write code to return response in XML or HTML instead of JSON, and add it to configuration.One thing to keep in mind is that ScrapyRT was not developed with long crawls in mind.Remember: after sending a request to ScrapyRT you have to wait for the spider to finish before you get response.So if the request requires the spider to crawl an enormous site and generates 1 million requests in callback, then ScrapyRT isn\u2019t the best option for you as you will likely have to sit in front of a blank screen waiting for your crawl to finish and return the item.One possible way of solving this problem would involve modifying ScrapyRT so that it could use websockets or HTTP push notifications - this way API could send items as they arrive in API.Currently data from spider is returned in response to initial request - so after sending each request you have to wait for the response until data is returned by spider. If you expect that your spider will generate lots of requests in callback but you actually don\u2019t need all of them you can limit the amount of requests by passing max_requests parameter to Scrapy.If you would like to learn more about ScrapyRT or contribute to the open source project, then check out the ScrapyRT  and .At Scrapinghub we specialize in turning unstructured web data into structured data. If you have a need to start or scale your web scraping projects then our  is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!\u00a0"}
{"contents": "Today, we\u2019re delighted to announce the launch of the beta program for\u00a0 for automated product and article extraction.After much development and refinement with alpha users, our team have refined this machine learning technology to the point that data extraction engine is capable of automatically identifying common items on product and article web pages and extracting them without the need to develop and maintain individual web crawlers for each site.Enabling developers to easily turn unstructured product and article pages into structured datasets at a scale, speed and flexibility that is nearly impossible to achieve when manually developing spiders.With the AI enabled data extraction engine contained within the developer API, you now have the potential to extract product data from 100,000 e-commerce sites without having to write 100,000 custom spiders for each.\u00a0As result, today we\u2019re delighted to announce the launch of the developer API's public beta.If you are interested in e-commerce or media monitoring and would like to get early access to the data extraction developer API then be sure to When you sign up to the beta program you will be issued an API key and documentation on how to use the API. From there you are free to use the developer API for your own projects and retain ownership of the data you extracted when the beta program closes.What's even better, the beta program is completely free. You will be assigned a daily/monthly request quota which you are free to consume as you wish.The beta program will run until July 9th, so if you\u2019d like to be involved then be sure to  as places are limited.Once you\u2019ve been approved to join the beta program and have received your API key, using the API is very straightforward.Currently, the API has a single endpoint: . A request is composed of one or more queries where each query contains a URL to extract from, and a page type that indicates what the extraction result should be (product or article).Requests and responses are transmitted in JSON format over HTTPS. Authentication is performed using HTTP Basic Authentication where your API key is the username and the password is empty.To make a request simply send a POST request to the API along with your API key, target URL and pageType (either article or product):Or, in Python:To facilitate query batching (see below) API responses are wrapped in a JSON array. Here is an article from our blog that we want to extract structured data from:And the response from the article extraction API:As mentioned previously the developer API is capable of extracting data from two types of web pages: and pages.The product extraction API enables developers to easily turn product pages into structured datasets for e-commerce monitoring applications.To make a request to the product extraction API, simply set the \u201cpageType\u201d attribute to \u201cproduct\u201d, and provide the URL of a product page to the API. Example:The product extraction API is able to extract the following data types:All fields are optional (can be null), except for url and probability.The article extraction API enables developers to easily turn articles into structured datasets for media monitoring applications.To make a request to the article extraction API, simply set the \u201cpageType\u201d attribute to \u201carticle\u201d, and provide the URL of an article to the API. Example:The article extraction API is able to extract the following data types:Similarly to the product extraction API, all article extraction fields are optional (can be null), except for url and probability.Both the product and article extraction API offer the ability to submit multiple queries (up to 100) in a single API request:The API will return the results of the extraction as the data extraction receives them, so query results are not necessarily returned in the same order as the original query.If you need an easy way to associate the results with the queries that generated them, you can pass an additional \"meta\" field in the query. The value that you pass will appear as a \"userMeta\" field in the corresponding query result. For example, you can create a dictionary keyed on the \"meta\" field to match queries with their corresponding results:If you would like to learn more about the developer API\u2019s functionality and how you can use it for your specific projects then check out the API documentation (will be sent to you when you sign up).The Developer API Beta Program is only open for a limited time (July 9th), so if you would like to get early and free access to the future of product and article extraction then be sure to "}
{"contents": "To accurately extract data from a web page, developers usually need to develop custom code for each website. This is manageable and recommended for tens or hundreds of websites and where data quality is of the utmost importance, but if you need to extract data from thousands of sites, or rapidly extract data from sites that are not yet covered by pre-existing code, this is often an insurmountable challenge.The complex and resource intensive nature of developing code for each individual website, acts as a bottleneck severely curtailing the scope of companies data extraction and analysis capabilities.Nowhere has this need for real time data extraction at scale being more needed than in e-commerce and media monitoring. Where the ability to monitor products on any online e-commerce store or monitor news from thousands of media outlets would take a company\u2019s business intelligence capabilities to a completely new level.Scrapinghub\u2019s new has been specifically designed for real-time e-commerce & article extraction at scale, and we\u2019re now .At the core of the  able to extract data from a web page without the need to design custom code. Through the use of deep learning, computer vision and Crawlera, Scrapinghub\u2019s advanced proxy management solution, the data engine is able to automatically identify common items on product and article web pages and extract them without the need to develop and maintain extraction rules for each site.With this AI technology, developers and companies now have the ability to extract product data from e-commerce sites without having to write custom data extraction code for each website.As with any machine learning based solution, the coverage and accuracy of the output is open to more inaccuracies compared to custom developed code.However, after much testing and refinement with alpha users, our data science team have improved our machine learning technology and operational processes to the point that the data extraction engine is capable of yielding commercially viable data quality for users.Key to this success, has been Scrapinghub\u2019s 10+ year experience being at the forefront of web scraping technologies and extracting over 8 billion pages per month. This experience and scale has enabled us to overcome a lot of the technical challenges faced by AI enabled data extraction engines and design a solution that is viable for commercial applications.Ideally suited for developers the API offers a flexible and highly scalable data extraction engine for large scale data analysis and visualisation applications. Especially:The AI enabled web scraping technology used as part of the API has the potential to unlock the web's full potential, turning the web into the world\u2019s largest structured database.Now instead of having to manually develop and maintain code for each new website, you can simply configure your applications to send it\u2019s queries to the developer API and receive structured data ready for analysis in response.Not only does this capability enable developer teams to build highly scalable data extraction capabilities, it also enables data science teams to rapidly prototype and test the value of data science projects, and stands as a backup to your existing custom built code if they were ever to break.Currently there are two versions of the API designed for two separate use cases:Although we are initially focused on providing the API for product and article extraction, overtime we plan to expand the types of data the API can automatically extract to include company/people profile data, real estate, reviews, etc. Further enhancing the accessibility of the web\u2019s data.If you are interested in e-commerce or media monitoring and would like to get early access to the data extraction developer API then be sure to sign up to the public beta programWhen you sign up you will be issued an API key, along with documentation on how to use the API. From there you are free to use the developer API for your own projects and retain ownership of the data you extracted when the beta program closes.What's even better, there is zero cost involved with the beta program. You will be assigned a daily/monthly request quota which you are free to consume as you wish."}
{"contents": "Football. From throwing a pigskin with your dad, to crunching numbers to determine the probability of your favorite team winning the Super Bowl, it is a sport that's easy to grasp yet teeming with complexity. From game to game, the amount of complex data associated with every team - and every player - increases, creating a more descriptive, timely image of the League at hand.By using web scraping and data analytics, the insights that we, the fans, crave - especially during the off-season - become accessible. Changing agreements, draft picks, and all the juicy decisions made in the off-season generate a great deal of their own data.By using a few simple applications and tricks we can see the broader patterns and landscapes of the sport as a whole. Salary data is especially easy to scrape, and for the purposes of this article we\u2019ll build a scraper to extract this data from the web, then teach you how to use simple data analytics to answer any questions we might have.Amid this sea of data, interesting insights appear, like the correlation between age and salary, who gets the biggest and smallest paycheck, or which position brings in the most cash. To get the information we need to make these conclusions, we\u2019re going to extract NFL players\u2019 salary data from a public source. Then, using that data, we\u2019ll create adhoc reports to unearth patterns hidden in the numbers.In this article, guest writer Attila T\u00f3th, founder of ScrapingAuthority.com, will give you a bird\u2019s eye view of how to scrape, clean, store, and analyze unstructured public data.\u00a0In addition to the code, we\u2019ll demonstrate our thought process as we go through each step. Whether you want to build a scrapy spider or only work on data analysis, there\u2019s something here for everybody.The site we\u2019re going to scrape is , a site that has all kinds of information about NFL player contracts and salaries.For this project we will use the Scrapy web scraping framework for data extraction; a MySQL database to store said data; a pandas library to work with data, and the tried-and-true matplotlib for charting.We will use the follow process to complete the project:Here is example URL of the quarterback page we want to extract data from:However, we will be extracting data for all positions.Our spiders will be designed to extract data from the following fields: , .As always, we first need to inspect the website to be able to extract the target data.\u00a0As we can see, each field is inside a separate tag which makes it easy to extract the data. An important thing to keep in mind is that all the fields inside a row tag belong to only one item in our spider. Now let\u2019s figure out how to actually fetch these fields.\u00a0Let\u2019s kick off with starting a new Scrapy project:In order to fetch the data with Scrapy, we need an item defined in the  file. We will call it ContractItem:\u00a0Now we\u2019re done with defining the item, let's start coding the spider.There are two challenges we need to overcome when designing our spider:There are multiple ways to implement this spider, however, for this project we decided to use an approach which is the most straightforward and simple.First, create a new spider python file and inside the file a class for our spider:Then define  and :\u00a0Now we need to implement the  function. Inside this function we usually populate the item. However, in this case we first need to make sure that we\u2019re grabbing data from all the position pages (quarterback, running-back, etc.).In the parse function we are doing exactly that: finding the URLs and requesting each of them so we can parse them later:So what does our parse function actually do here?The parse function iterates over each dropdown element (positions). Then inside the iteration we extract the URL for the actual page where we want to get data from. Finally, we request this page and pass the position info as metadata for the  function, which will be coded next.The  function will need to deal with actual data extraction and item population. As we previously learned from inspecting the website, we know that each table row contains one item. So we\u2019ll iterate over each row to populate each item. Easy right?But remember that we want the data to be written into a database at the end. To simplify that, we\u2019re using item loaders and i/o processors to clean the extracted data.The next step is to implement the input processors which will clean our freshly extracted data.Let\u2019s make use of item loader input and output processors. There are two main tasks that the processors need to accomplish:  and . Removing html tags is important because we don\u2019t want messy data in our database.The conversion part is important because our database table, which we will write into, will have number and text columns as well. And so we need to make sure that we are inserting actual numbers as number fields and cleaned text data as text fields.The fields that need processing (other than removing html tags) include: a.As all of these fields are number fields they need further processing:At this point our extracted data is clean and ready to be stored inside a database. But before that, we need to remember that we likely have plenty of duplicates because some players hold multiple positions. For example, a player can be a safety and a defensive-back at the same time. So this player would be in the database multiple times. We don\u2019t want redundant data corrupting our data set, so we need to find a way to filter duplicates whilst scraping.The solution for this is to write an item pipeline which drops the actual item if the player has already been scraped. Add this pipeline to our  file:This pipelines works by continuously saving scraped player names in, which is a set. If the player has already been processed by the spider it raises a warning message and simply drops (ignores) the item. If the currently processed player has not been processed yet then no need to drop the item.One of the best ways to write a database pipeline in Scrapy is as follows:2. Create new pipeline class and a constructor to initiate db connection.3. Overridefunction to get db settings.4. Insert item into database5. Close db connection when the spider finishesWe\u2019ve extracted data. Cleaned it. And saved it into a database. Finally, here comes the  part: trying to get some actual insights into NFL salaries.Let\u2019s see if we find something interesting. For each report, we\u2019re going to use pandas dataframes to hold the queried data together.Now, let\u2019s create some ad hoc reports using the database we\u2019ve generated to answer some simple questions, like \u201cWho is the youngest player?\u201d and \u201cWhen will the longest active contract end?\u201dTo do so, let\u2019s fetch all the data into a dataframe. We will need to create a new dictionary based on the dataframe to get min, max, mean and median values for all the data fields. Then create a new dataframe from the dict to be able to display it as a table.This table contains some basic descriptive information about our data.Here lets query the data set to identify which players have the highest and lowest salaries:The minimum yearly salary for an NFL player is $480,000 (in 2019 season it\u2019s going to be $15,000 more). Tanner Lee, 24, QB for the Jags, is the only player who got this amount of money, in his rookie season. For some unknown reason, other teams stick to $487,500 when it comes to minimum payment.The top five players with the biggest paycheck are all QBs, which is not surprising. Aaron Rodgers is the 1st with a yearly average salary of $33,500,000. The youngest in the top five is Jimmy Garoppolo, 28, with a solid $28,000,000 average yearly salary. Except Kirk Cousins, they all have long-term contracts (4+ years remaining).Next lets see which players have the longest contracts until they become a free-agent:There are four players whose teams decided they want to tie their players down to long term contracts. 3 out of 4 are defensive players. 2 out of 4 are linemen. The top 3 players are 28 or 29 years old. Landon Collins is the only player who is younger, he\u2019s only 25 but has been in the NFL since 2015.Next, let's look at the contract length distribution across players to see what is the average active contract length:\u00a0What this chart really shows is that currently, a huge amount of players\u2019 contracts will end next year and if nothing happens before then they\u2019ll become free agents. Almost 1,000 players have a year remaining in their contracts. More than 600 have 2 years left. Less than 400 players have 3 years left, and just a handful of players have 4, 5 or 6 year contracts.To dig a little deeper we then looked at the correlation between a players age and their contract length:This graph shows the correlation between a player\u2019s age and the remaining years of his contract (contract length). There\u2019s a downhill trend between age 22 and 26, which probably suggests that most of players of this age still have their rookie contracts. At age 26 they start negotiating new contracts hence the uphill climb. The average rookie contract is a 3-year deal but after that an average player receives a 1 or 2-year contract.Most of the players are either 24, 25 or 26 years old. 26 is the average and median as we mentioned it before.\u00a0Some quick remarks:Now, this is a really simple chart. It appears that the more years left in a player\u2019s contract (the longer the contract,) the more average yearly salary he gets. There's something special that happens after the 3 year mark because there\u2019s a huge pay rise if the contract is longer than 3 years.We covered website inspection, data extraction, cleaning, processing, pipelines and descriptive analysis. If you\u2019ve read this far I hope this article has given you some practical tips on how to leverage web scraping. Web scraping is all about giving you the opportunity to gain insights and make data-driven decisions. Hopefully this tutorial will inspire future projects and exciting new insights.At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.\u00a0If you have a need to start or scale your web scraping project then ourare available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on.Thanks for reading!The full source code of this tutorial is available on Github-----------------------------------------------------------------------------------------------------------------------------\u00a0Founder of  where he teaches web scraping and data engineering. Expertise in designing and implementing web data extraction and processing solutions.\u00a0\u00a0\u00a0\u00a0"}
{"contents": "In this the second post in our solution architecture series, we will share with you our step-by-step process for data extraction requirement gathering.As we mentioned in the first post in this series, the ultimate goal of the requirement gathering phase is to , if possible to have zero assumptions about any variable so the development team can build the optimal solution for the business need.As a result, accurately defining project requirements most important part of any web scraping project.In this article we will discuss the four critical steps to scoping every web scraping project and the exact questions you should be asking yourself when planning your data extraction needs.The requirement gathering process can be broken into two parts: 1) understanding the business needs, and 2) defining the technical requirements to meet those needs.The business need is at the core of every web scraping project, as it clearly defines what objective do they want to achieve.Once, you\u2019ve clearly defined what you\u2019d like to achieve then it is possible to start gathering the technical requirements. At the start of every solution architecture process, our team will put a huge amount of focus on trying to understand our customers underlying business needs and objectives. Because when we truly understand your objectives, we can assist you in selecting the best data sources for your project and be able to best optimise the project if there is any rescoping or trade offs required due to technical or budgetary constraints.Questions to ask yourself are:On the surface these questions may seem very simple, but we\u2019ve found that in a lot of cases customers come to use without a clear idea of what web data they need and how they can use it to achieve their business objectives. So the first step is to clearly identify what type of web data you need to achieve your business objectives.Once, everyone has a clear understanding of the business objectives, then it is time to define the technical requirements of the project i.e. how do we extract the web data we need.There are four key parts to every web scraping project:We will look at each one of these individually, breaking down why each one is important and the questions you should be asking yourself at each stage.The first step of the technical requirement gathering process is defining what data needs extracting and where can it be found.Without knowing what web data you need and where it can be found most web scraping projects aren\u2019t viable. As a result, getting a clear idea of these basic factors are crucial for moving forward with the project.Questions to ask yourself are:By asking yourself these questions you will be able to really start to define the scope of the project.The next step in the requirement gathering process is digging into extracting the data\u2026During this step our goal is to clearly capture what data do we want to extract from the target web pages. Oftentimes, there is vast amounts of data available on a page so the goal here is to focus in on the exact data the customer wants.One of the best methods to clearly capturing the scope of the data extraction is to take screenshots of the target web pages and mark with fields need to be extracted. Oftentimes during calls with our solution architects we will run through this process with customers to ensure everyone understands exactly what data is to be extracted.Questions to ask yourself are:A general rule of thumb is that the more data being extracted from a page, the more complex the web scraping project. Every new data type will require additional data extraction, data quality assurance checks, and maybe more technical resources in certain circumstances (i.e. a headless browser if a data type is rendered via javascript, or if multiple requests are required to access the target data).Once this step is complete we then look to estimate the scale of the project...Okay, by this stage you should have a very good idea of the type of data you want to extract and how your crawlers will find and extract it. Next, our goal is to determine the scale of the web scraping project.This is an important step as it allows you to estimate the amount of infrastructural resources (servers, proxies, data storage, etc.) you\u2019ll need to execute the project, the amount of data you\u2019re likely to receive and the complexity of the project.The three big variables when estimating the scale of your web scraping projects are the:After following steps 1 & 2 of the requirement gathering process, you should know exactly how many websites you\u2019d like data extracted from (variable 1). However, estimating the number of records that will be extracted (variable 2) can be a bit trickier.In some cases (often in smaller projects), it is just a matter of counting the number of records (products, etc.) on a page and multiply by the number of pages. In certain situations the website will even list the number of records on a website.However, there is no one size fits all solution for this question. Sometimes it is impossible to estimate the number of results you will extract, especially when the crawl is extracting hundreds of thousands or millions of records. In cases like these often the only way to know for sure how many records the crawl will return is by actually running the crawl. You can guesstimate, but you\u2019ll will never know for sure. The final factor to take into account is how often would you like to extract this data? Daily, weekly, monthly, once off, etc? Also, do you need the data extraction completed within a certain time window to avoid changes to the underlying data?It goes without saying that the more often you extract the data, the larger the scale of the crawl. If you go from extracting data monthly to extracting data daily you are in effect multiplying the scale of the crawl by a factor of 30.In most circumstances, increasing the scale from monthly to daily won\u2019t have much of an effect other than increasing the infrastructural resources required (server bandwidth, proxies, data storage, etc.). However, it does increase the risk that your crawlers will be detected and banned, or put excessive pressure on the websites servers. Especially, if the website typically doesn\u2019t receive a large volume of traffic each day. Sometimes the scale of the crawl is just too big to complete at the desired frequency without crashing a website or requiring huge infrastructural resources.Typically this is only an issue when extracting enormous quantities of data on an hourly or less basis, such as monitoring every product on an e-commerce store in real-time. During the technical feasibility phase we normally test for this. The other factor to take into account when moving from a once-off data extraction to a recurring project is how do you intend to process the incremental crawls? Here you have a number of options:The crawlers can be configured to do this or else they can just extract all the available data during each crawl and you can post-process it to your requirements afterwards.Finally, the last step of the project scoping process is defining how do you want to interact with the web scraping solution along with how do you want the data delivered.If you are building the web scraping infrastructure yourself, you really only have one option: you\u2019re managing the data, the web scraping infrastructure and the underlying source code. However, when customers work with Scrapinghub (or other web scraping providers) we can offer them a number of working relationships to best meet their customers needs. Here are some of Scrapinghub\u2019s most common working relationships:The next question is in what format do you want the data and how you would like the data delivered. These questions are largely dependant on how you would like to consume the data and the nature of your current internal systems. There are a huge range of options for both, but here are some examples:These considerations are typically more relevant when you are working with an external web scraping partner.So there you have it, they are the four steps you need to take to define the scope of your web scraping project. In the next article in the series we will share with you how to take that project scope and conduct a legal review of the project. At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.If you have a need to start or scale your web scraping project then our  are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on."}
{"contents": "For many people (especially non-techies), trying to architect a web scraping solution for their needs and estimate the resources required to develop it, can be a tricky process.Oftentimes, this is their first web scraping project and as a result have little reference experience to draw upon when investigating the feasibility of a data extraction project.In this series of articles we\u2019re going to break down each step of Scrapinghub\u2019s four step solution architecture process so you can better scope and plan your own web scraping projects.At Scrapinghub we have a full-time team of solution architects who architect over 90 custom web scraping projects each week for everything from e-commerce and media monitoring, to lead generation and alternative finance use cases. So odds are if you are thinking of investigating a web scraping project our team has already architected a solution for something very similar.As a result, throughout this series we will be sharing with you the exact checklists and processes we use, along with some insider tips and rules of thumb that will make investigating the feasibility of your projects much easier.In this article, the first in the series, we\u2019re going to give you a high level overview of our solution architecture process so you replicate it for your own projects.The ultimate goal of the requirement gathering phase is to , if possible to have zero assumptions about any variable so the development team can build the optimal solution for the business need.This is a critical process for us at Scrapinghub when working on customer projects so we can ensure we are developing a solution that meets their business need for web data, manage expectations and reduce risk in the project.However, the same is true if you are developing a web scraping infrastructure for your projects. Accurately capturing the project requirements will allow your development team to\u00a0ensure your web scraping project precisely meets your overall business goals.Here you want to capture two things:It is critical for you and/or your business that your data requirements accurately match your underlying business goals and your need for the data. A constant supply of high quality data can give your business a huge competitive edge in the market, but what is important is .It is very easy to extract data from the web, what\u2019s difficult is extracting the right data at a frequency and data quality that makes it useful for your business processes.With every customer we speak with, we dive deep into their underlying business goal to better understand not only their specific data requirements, but also why they want the data and how it fits into the bigger picture. Because oftentimes, our team of solution architects are able to work with them to find the alternative or additional data sources for their specific requirements that better suit their business goals.However, when investigating the feasibility of any web scraping project you should always be trying to clarify:In this article (coming soon), we will walk you through the exact steps our team uses to gather project requirements and scope the best possible solution to meet them.The second step of any solution architecture process is to check if there are any legal barriers to extracting this data.With the increased level of awareness about data privacy and web scraping in the last number of years, ensuring your web scraping project is legally compliant is now a must. Otherwise you could land you or your company in a lot of bother.In this article (coming soon), we will share with you our exact legal assessment checklist that our solution architecture team uses to review every project request we receive. Our legal team has created a best practice guide for the solution architecture team to utilise so they know when to flag a project with legal for a review. Once flagged with legal, our legal team will review the project based on the criteria below, as well as others, to determine if we are able to proceed with the project.However, in general you need to be assessing your project against the following criteria:If your answers to any of the above questions raise concerns, you should be ensuring that a thorough legal review of the issue is conducted prior to scraping. Once you've completed this review then you are in a good position to move forward to assessing the technical feasibility and architecting your web scraping solution.Assuming your data collection project passed the legal review, the next step in the solution architecture process is to assess the technical feasibility of executing the project successfully.This is a critical step in our solution architecture process and a step most independent developers working on their own or for their company\u2019s projects overlook.There is a strong tendency amongst developers and business leaders to start developing their solution straight away. For simple projects this often isn\u2019t an issue, however, for more complex projects, developers can quickly discover that they run into a brick wall and can\u2019t overcome the challenges.We\u2019ve found that a bit of upfront testing and planning, can save countless wasted man hours down the line if you start developing a fully featured solution only to hit a technical brick wall.During the technical review phase, one of our solution architects will examine the website and run a series of small scale test crawls to evaluate the technical feasibility of developing a solution that meets the customers requirements (crawl speed, coverage and budgetary requirements).These tests are primarily designed to determine the difficulty of extracting data from the site, will there be any limitations on crawl speed & frequency, is the data easily discoverable, is there any post-processing or data science requirements, does the project require any additional technologies, etc.Once complete, this technical feasibility review gives the solution architect the information they need to firstly determine if the project is technically feasible and then what is the optimal architecture for the solution.In this article (coming soon), we\u2019ll give you a behind the scenes look at some of the tests we carry out when assessing the technical feasibility of our customers projects.The final step in the process is architecting the solution and estimating the technical and human resources required to deliver the project.Oftentimes, the solution need to be approached and scoped in phases, to balance the tradeoff of timeline, budget, and technical feasibility. Our team will propose the best first step to tackle your project while keeping the bigger goal in mind.Here you need to architect a web scraping infrastructure using the following building blocks:In this article (coming soon), we share the process we use to architect a solution, give examples of real solutions we have architected and the resources required to execute them.Once a solution has been architected and the resources estimated, our team has all the information they need to present the solution to the customer, estimate the cost of the project and draft a statement of work capturing all their requirements and the proposed solution.At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.If you have a need to start or scale your web scraping project then our  are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on.\u00a0\u00a0"}
{"contents": "In the fourth post of this , we will share with you our step-by-step process for evaluating the technical feasibility of a web scraping project.After completing the legal review of a potential project, the next step in every project should be to assess the technical feasibility and complexity of executing the project successfully.A bit of upfront testing and planning can save countless number of wasted hours down the line, if you start developing a fully featured solution only to hit a technical brick wall.The technical review process focuses on the four key parts of the web scraping development process:We will look at each one of these individually, breaking down the technical questions you should be asking yourself at each stage.Using the project requirements we gathered in the requirement gathering process you should have all the information you need to start accessing the technical feasibility of the project.The first step of the technical feasibility process is investigating whether it is possible for your crawlers to accurately discover the desired data as defined in the project requirements.For most projects, where you know the exact websites you want to extract data from and can manually navigate to the desired data there are usually no technical challenges in developing crawlers to discover this data. The crawler just needs to be designed to replicate the user behaviour sequence you need to execute to access the data.Typically, the main reasons for experiencing technical challenges at the data discovery phase is if you don\u2019t know how to discover all the data manually yourself:For example, the most common technical challenge we run into during the data discovery stage is when the client would like to extract a specific type of data but either or .In cases like these, we can investigate the viability of discovering the data they require using a number of approaches we have at our disposal:Each of these approaches have their limitations, most notably data quality and coverage. As a result, we generally recommend that we develop customer crawlers for each website if data quality is of priority.The next step is to verify the technical feasibility of the data extraction phase.Here the focus is on verifying that the data can be accurately extracted from the target websites and give an assessment on the complexity required. Our solution architecture team will do a series of tests to enable them to design the extraction process and verify that it is technically possible to extract the data at the required quality. These tests will test for:Once this step is complete the solution architect will then investigate the feasibility of extracting the data at the required scale...With the ability to discover and extract the data on a small scale verified then the next step is to verify that the project can be executed at the required scale & speed to meet the project requirements. This is often the most difficult and troublesome area when it comes to web scraping and the area where a good crawl engineers experience and expertise really shines.Provided there are no legal or glaring data discovery/extraction issues it is normally feasible to develop a small scale crawler to extract data without running into any issues. However, as the scale and speed requirements of the crawl increases you can quickly run into trouble:With this information, our solution architecture team is able to get a deep understanding of the complexity of the project and the difficulty of delivering and maintaining it at scale.The final technical feasibility step is verifying that the data delivery format and method is viable. In most cases, there are very few issues at this stage as we have numerous data format and delivery methods available to us to satisfy any customer requirement.However, certain options might be more complex than others as some require more development time (example: develop a custom API, etc.).The most common complexity adding step is if the project requires data post-processing or data science to meet its requirements. These can significantly increase the complexity of the project.So there you have it, they are the four steps to conducting a technical feasibility review of your web scraping project. In the next article in the series, we will share with you how we take the project requirements and the complexity analysis and develop a custom solution.At Scrapinghub we have extensive experience architecting and developing data extraction solutions for every possible use case.Our legal and engineering teams work with clients to evaluate the technical and legal feasibility of every project and develop data extraction solutions that enable them to reliably extract the data they need.If you have a need to start or scale your web scraping project then our  are available for a free consultation, where we will evaluate and architect a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and any questions you might have. So please leave a comment below with what you thought of the article and what you are working on.\u00a0"}
{"contents": "Visual web scraping tools are great. They allow people with little to no technical know-how to extract data from websites with only a couple hours of upskilling, making them great for simple lead generation, market intelligence and competitor monitoring projects. Removing countless hours of manual entry work for sales and marketing teams, researchers, and business intelligence team in the process.However, no matter how sophisticated the creators of these tools say their visual web scraping tools are, users often run into issues when trying to scrape mission critical data from complex websites or when scraping the web at scale.In this article, we\u2019re going to talk about the biggest issues companies face when using visual web scraping tools like Mozenda, Import.io and Dexi.io, and what they should do when they are no longer fit for purpose.First, let\u2019s use a commonly known comparison to help explain the pros and cons of visual web scraping tools versus manually coding your own web crawlers.If you have any experience of developing a website for your own business, hobby or client projects, odds are you have come across one of the many online tools that say you can create visually stunning and fully featured websites using a simple-to-use visual interface.When we see their promotional videos and the example websites their users have \u201ccreated\u201d on their platforms we believe we have hit the jackpot. With a few clicks of a button, we can design a beautiful website ourselves at a fraction of the cost of hiring a web developer to do it for us. Unfortunately, in most cases these tools never meet our expectations.No matter how much they try, visual point and click website builders can never replicate the functionality, design and performance of a custom website created by a web developer. Websites created by visual website builder tools are often slow, inefficient, have poor SEO and severely limit the translation of design requirements into the desired website. As a result, outside of very small business websites and rapid prototyping of marketing landing pages, companies overwhelming have professional web developers design and develop custom websites for their businesses.   The same is true of visual point and click web scraping tools. Although the promotional material of many of these tools make it look like you can extract any data from any website at any scale, in reality this is often never true.  Like visual website builder tools, visual web scraping tools are great for small and simple data extraction projects where lapses in data quality or delivery aren\u2019t critical, however, when scraping mission critical data from complex websites at scale then they quickly suffer some serious issues often making them a bottleneck in companies data extraction pipelines and a burden on their teams. With that in mind we will look at some of these performance issues in a bit more detail... Visual point and click web scraping tools suffer from similar issues that visual website builders encounter. Because the crawler design needs to be able to handle a huge variety of website types/formats and isn\u2019t being custom developed by an experienced developer, the underlying code can sometimes be clunky and inefficient. Impacting the speed at which visual crawlers can extract the target data and make them more prone to breaking.Oftentimes, these crawlers make additional requests that aren\u2019t required, render JavaScript when there is no need, and increase the footprint of the crawler increasing the likelihood of your crawlers being detected by anti-bot countermeasures.These issues often have little noticeable impact on small scale and infrequent web scraping projects, however, as the volume of data being extracted increases, users of visual web scrapers often notice significant performance issues in comparison to custom developed crawlers.Unnecessarily, putting more strain on the target websites servers, increasing the load on your web scraping infrastructure and make extracting data within tight time windows unviable.Visual web scraping tools also suffer from increased data quality and reliability issues due to the technical limitations described above along with their inherent rigidity, lack of quality assurance layers and the fact their opaque nature makes it harder to identify and fix the root causes of data quality issues. These issues combine to reduce the overall data quality and reliability of data extracted with visual web scraping tools and increase the maintenance burden.  Another drawback of visual web scraping tools is the fact that they often struggle to handle modern websites that make extensive use of JavaScript and AJAX. These limitations can make it difficult to extract all the data you need and simulate user behaviour adequately.It can often also be complex to next to impossible to extract data from certain types of fields on websites, for example: hidden elements, XHR requests and other non-HTML elements (for example PDF or XLS files embedded on the page).For simple web scraping projects these drawbacks might not be an issue, but for certain use cases and sites they can make extracting the data you need virtually impossible.Oftentimes, the technical issues described above aren\u2019t that evident for smaller scale web scraping projects, however, they can quickly become debilitating as you scale up your crawls. Not only do they make your web scraping processes more inefficient and buggy, they can stop you from extracting your target data entirely.Increasingly, large websites are using anti-bot countermeasures to control the way automated bots access their websites. However, due to the inefficiency of their code, web crawlers designed by visual web scraping tools are often easier to detect than properly optimised custom spiders. Custom spiders can be designed to better simulate user behaviour, minimise their digital footprint and counteract the detection methods of anti-bot countermeasures to avoid any disruption to their data feeds.In contrast, the same degree of customisation is often impossible to replicate with crawlers built using visual web scraping tools without getting access to and modifying the underlying source code of the crawlers. Which can be difficult to do as it is often proprietary to the visual website builder.  As a result, often the only step you can take is to increase the size of your proxy pool to cope with the increasing frequency of bans, etc. as you scale.If you are using a visual web scraping tool with zero issues and have no plans to scale your web scraping projects then you might as well just keep using your current web scraping tool. You likely won\u2019t get any performance boost from switching to custom designed tools.Although current visual web scraping tools have come along way, currently they often can\u2019t replicate the accuracy and performance of custom designed crawlers, especially when scraping at scale. In the coming years, with the continued advancements in artificial intelligence these crawlers may be able to match their performance. However for the time being, if your web scraping projects are suffering from poor data quality, crawlers breaking, difficulties scaling, or want to cut your reliance on your current providers support team then you should seriously consider building a custom web scraping infrastructure for your data extraction requirements. In cases like these, it is very common for companies to contact Scrapinghub to migrate their web scraping projects from a visual web scraping tool to a custom web scraping infrastructure.  Not only are they able to significantly increase the scale and performance of your web scraping projects, they no longer have to rely on proprietary technologies, have no vendor lock-in, and have more flexibility to get the exact data they need with no data quality or reliability issues. Removing all of the bottlenecks and headaches companies normally face when using visual web scraping tools.If you think it is time for you to take this approach with your web scraping, then you have two options:At Scrapinghub, we can help you with both options. We have a  to help development teams build, scale and manage their spiders without all the headaches of managing the underlying infrastructure. Along with a range of  where we develop and manage your custom high performance web scraping infrastructure for you.  If you have a need to start or scale your web scraping projects then our  is available for a free consultation, where we will evaluate and develop the architecture for a data extraction solution to meet your data and compliance requirements.At Scrapinghub we always love to hear what our readers think of our content and would be more than interested in any questions you may have. So please, leave a comment below with your thoughts and perhaps consider sharing what you are working on right now!"}
